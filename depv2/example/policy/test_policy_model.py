#!/usr/bin/env python3
"""
Thunder Policy Model æµ‹è¯•ç¨‹åº
æµ‹è¯• exported/policy.pt æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸åŠ è½½å’Œæ¨ç†
"""

import os
import sys
import time
import numpy as np
import torch
from pathlib import Path

def test_policy_model(model_path: str):
    """æµ‹è¯•ç­–ç•¥æ¨¡å‹"""
    
    print("ğŸ¤– Thunder Policy Model Test")
    print("=" * 60)
    
    # 1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    print(f"ğŸ“ Checking model file: {model_path}")
    if not os.path.exists(model_path):
        print(f"âŒ Model file not found: {model_path}")
        return False
    
    file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
    print(f"âœ… Model file found, size: {file_size:.2f} MB")
    
    try:
        # 2. åŠ è½½æ¨¡å‹
        print(f"\nğŸ”„ Loading PyTorch JIT model...")
        model = torch.jit.load(model_path)
        model.eval()
        print(f"âœ… Model loaded successfully")
        
        # 3. æµ‹è¯•æ¨¡å‹è¾“å…¥è¾“å‡ºç»´åº¦
        print(f"\nğŸ” Testing model dimensions...")
        
        # æ ¹æ®thunder_flat_deploy.pyçš„é…ç½®ï¼Œè§‚æµ‹ç»´åº¦åº”è¯¥æ˜¯57
        expected_obs_dim = 57
        expected_action_dim = 16
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(1, expected_obs_dim, dtype=torch.float32)
        print(f"   Input shape: {test_input.shape}")
        
        # 4. æ‰§è¡Œæ¨ç†
        print(f"\nâš¡ Running inference...")
        start_time = time.time()
        
        with torch.no_grad():
            test_output = model(test_input)
        
        inference_time = (time.time() - start_time) * 1000  # ms
        print(f"âœ… Inference completed in {inference_time:.2f} ms")
        print(f"   Output shape: {test_output.shape}")
        
        # 5. éªŒè¯è¾“å‡ºç»´åº¦
        if test_output.shape[0] != 1:
            print(f"âŒ Expected batch size 1, got {test_output.shape[0]}")
            return False
        
        if test_output.shape[1] != expected_action_dim:
            print(f"âŒ Expected action dimension {expected_action_dim}, got {test_output.shape[1]}")
            return False
        
        print(f"âœ… Output dimensions correct: {test_output.shape}")
        
        # 6. æ£€æŸ¥è¾“å‡ºæ•°å€¼èŒƒå›´
        output_values = test_output.squeeze(0).numpy()
        print(f"\nğŸ“Š Output analysis:")
        print(f"   Min value: {output_values.min():.4f}")
        print(f"   Max value: {output_values.max():.4f}")
        print(f"   Mean: {output_values.mean():.4f}")
        print(f"   Std: {output_values.std():.4f}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªåŠ¨ä½œå€¼
        print(f"   First 8 actions: {output_values[:8]}")
        
        # 7. æ€§èƒ½æµ‹è¯•
        print(f"\nâ±ï¸  Performance test (100 inferences)...")
        times = []
        
        for i in range(100):
            start = time.time()
            with torch.no_grad():
                _ = model(test_input)
            times.append((time.time() - start) * 1000)
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        max_time = np.max(times)
        min_time = np.min(times)
        
        print(f"   Average inference time: {avg_time:.2f} Â± {std_time:.2f} ms")
        print(f"   Min: {min_time:.2f} ms, Max: {max_time:.2f} ms")
        print(f"   Theoretical max frequency: {1000/avg_time:.1f} Hz")
        
        # 8. æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
        print(f"\nğŸ”¢ Testing different batch sizes...")
        batch_sizes = [1, 4, 8, 16]
        
        for batch_size in batch_sizes:
            test_batch = torch.randn(batch_size, expected_obs_dim, dtype=torch.float32)
            
            start = time.time()
            with torch.no_grad():
                batch_output = model(test_batch)
            batch_time = (time.time() - start) * 1000
            
            if batch_output.shape != (batch_size, expected_action_dim):
                print(f"   âŒ Batch {batch_size}: Wrong output shape {batch_output.shape}")
            else:
                time_per_sample = batch_time / batch_size
                print(f"   âœ… Batch {batch_size}: {batch_time:.2f} ms total, {time_per_sample:.2f} ms/sample")
        
        # 9. æ¨¡æ‹Ÿè§‚æµ‹æ•°æ®æµ‹è¯•
        print(f"\nğŸ¯ Testing with realistic observation data...")
        
        # åˆ›å»ºæ›´çœŸå®çš„è§‚æµ‹å‘é‡ï¼ˆæ¨¡æ‹Ÿå®é™…æœºå™¨äººçŠ¶æ€ï¼‰
        realistic_obs = create_realistic_observation()
        realistic_input = torch.tensor(realistic_obs, dtype=torch.float32).unsqueeze(0)
        
        with torch.no_grad():
            realistic_output = model(realistic_input)
        
        realistic_actions = realistic_output.squeeze(0).numpy()
        print(f"   Realistic observation input shape: {realistic_input.shape}")
        print(f"   Realistic actions output: {realistic_actions}")
        
        # æ£€æŸ¥åŠ¨ä½œæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…ï¼ˆé€šå¸¸æ¨¡å‹è¾“å‡ºåº”è¯¥åœ¨[-1, 1]èŒƒå›´å†…ï¼‰
        if np.all(np.abs(realistic_actions) <= 10.0):  # å®½æ¾çš„èŒƒå›´æ£€æŸ¥
            print(f"   âœ… Actions in reasonable range")
        else:
            print(f"   âš ï¸  Some actions might be out of normal range")
        
        print(f"\nğŸ‰ All tests passed! Model is ready for deployment.")
        return True
        
    except Exception as e:
        print(f"\nâŒ Error during model testing: {e}")
        import traceback
        traceback.print_exc()
        return False


def create_realistic_observation():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„çœŸå®è§‚æµ‹æ•°æ®"""
    
    # è§‚æµ‹å‘é‡ç»„æˆï¼ˆåŸºäºthunder_flat_deploy.pyçš„é…ç½®ï¼‰ï¼š
    # - base_ang_vel (3) + projected_gravity (3) = 6
    # - velocity_commands (3) = 3  
    # - joint_pos_rel (16) = 16
    # - joint_vel (16) = 16
    # - last_actions (16) = 16
    # æ€»è®¡: 6 + 3 + 16 + 16 + 16 = 57
    
    observation = np.zeros(57)
    
    # åŸºåº§è§’é€Ÿåº¦ (IMUæ•°æ®)
    observation[0:3] = np.array([0.1, -0.05, 0.2])  # å°å¹…æ‘†åŠ¨
    
    # æŠ•å½±é‡åŠ›å‘é‡
    observation[3:6] = np.array([0.0, 0.0, -1.0])  # å‘ä¸‹çš„é‡åŠ›
    
    # é€Ÿåº¦æŒ‡ä»¤
    observation[6:9] = np.array([1.0, 0.0, 0.0])  # å‰è¿›1m/s
    
    # å…³èŠ‚ç›¸å¯¹ä½ç½®ï¼ˆç›¸å¯¹äºé»˜è®¤å§¿æ€çš„åå·®ï¼‰
    # è…¿éƒ¨å…³èŠ‚ï¼ˆ12ä¸ªï¼‰+ è½®å­å…³èŠ‚ï¼ˆ4ä¸ªï¼Œåº”è¯¥ä¸º0ï¼‰
    joint_pos_rel = np.zeros(16)
    joint_pos_rel[0:12] = np.random.normal(0, 0.1, 12)  # å°å¹…éšæœºåå·®
    joint_pos_rel[12:16] = 0.0  # è½®å­ä½ç½®ä¸º0
    observation[9:25] = joint_pos_rel
    
    # å…³èŠ‚é€Ÿåº¦
    joint_vel = np.zeros(16)
    joint_vel[0:12] = np.random.normal(0, 0.5, 12)  # è…¿éƒ¨å…³èŠ‚é€Ÿåº¦
    joint_vel[12:16] = np.array([2.0, 2.0, 2.0, 2.0])  # è½®å­è½¬åŠ¨
    observation[25:41] = joint_vel
    
    # ä¸Šä¸€æ­¥åŠ¨ä½œ
    last_actions = np.random.uniform(-0.5, 0.5, 16)
    observation[41:57] = last_actions
    
    return observation


def main():
    """ä¸»å‡½æ•°"""
    model_path = "/home/ubuntu/Desktop/dog_deploy/deployment/exported/policy.pt"
    
    print("Testing Thunder policy model...")
    print(f"Model path: {model_path}")
    
    success = test_policy_model(model_path)
    
    if success:
        print(f"\nâœ… Model test completed successfully!")
        print(f"The model is ready for use with thunder_flat_deploy.py")
    else:
        print(f"\nâŒ Model test failed!")
        print(f"Please check the model file and try again.")


if __name__ == "__main__":
    main() 